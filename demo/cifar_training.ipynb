{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acshm9sK-WN1"
      },
      "source": [
        "# Training models on CIFAR-10/100 datasets, using ***torchdistill***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-an0T-1XAX5g"
      },
      "source": [
        "## 1. Make sure you have access to GPU/TPU\n",
        "Google Colab: *Runtime* -> *Change runtime type* -> *Hardware accelarator*: \"GPU\" or \"TPU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng_tsTQEr3sa",
        "outputId": "1787c5b4-379e-44cf-d99a-9003962b3247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb  5 12:37:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI0y3xO_-gkJ"
      },
      "source": [
        "## 2. Install ***torchdistill***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJSCmbJH_yuC",
        "outputId": "ebefb9da-0054-4b1f-bf87-14b765e79def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdistill\n",
            "  Downloading torchdistill-1.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdistill) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision<=0.16.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from torchdistill) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchdistill) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from torchdistill) (6.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchdistill) (1.11.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torchdistill) (3.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision<=0.16.0,>=0.15.1->torchdistill) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<=0.16.0,>=0.15.1->torchdistill) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.1.0,>=2.0.0->torchdistill) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.1.0,>=2.0.0->torchdistill) (1.3.0)\n",
            "Installing collected packages: torchdistill\n",
            "Successfully installed torchdistill-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdistill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekaJbQ3g-fa2"
      },
      "source": [
        "## 3. Clone ***torchdistill*** repository to use its example code and configuration files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYrQwcOa_r2P",
        "outputId": "49941ea0-e97f-4172-d512-5c7c5e03d37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdistill'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 127 (delta 0), reused 0 (delta 0), pack-reused 125\u001b[K\n",
            "Receiving objects: 100% (127/127), 247.33 KiB | 8.83 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Moon-kimchi/torchdistill.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0JxZZ4BAFpb"
      },
      "source": [
        "## 4. Train models on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMzEpVbNCtCE"
      },
      "source": [
        "Note that the hyperparameters of ResNet were chosen based on either train/val (splitting 50k samples into train:val = 45k:5k) or cross-validation, according to the original papers.  \n",
        "For the final run (once the hyperparameters are finalized), the authors used all the training images (50k samples).  \n",
        "- ResNet: https://github.com/facebookarchive/fb.resnet.torch\n",
        "\n",
        "The following examples demonstrate how to 1) tune hyperparameter and 2) do final-run with ResNet-20 on CIFAR-10 dataset, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx5VnugvS5nx"
      },
      "source": [
        "### 4.1 train:val = 45k:5k\n",
        "Let's start with a small model, ResNet-20, for tutorial.  \n",
        "\n",
        "Open `torchdistill/configs/sample/cifar10/ce/resnet20-hyperparameter_tuning.yaml` and update hyperparameters as you wish e.g., number of epochs (*num_epochs*), batch size (*batch_size* in *train_data_loader* entry), learning rate (*lr* within *optimizer* entry), and so on.\n",
        "By default, the hyperparameters in the example config are identical to those in the final run config.\n",
        "  \n",
        "You will find a lot of module names from [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [torchvision](https://pytorch.org/docs/stable/torchvision/) such as [`SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [`MultiStepLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiStepLR), [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), [`CIFAR10`](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.CIFAR10), [`RandomCrop`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomCrop) (, and more). You can update their parameters or replace such modules with other modules in the packages. For instance, `SGD` could be replaced with [`Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), and then you will change the parameters under `params` (at least delete `momentum` entry as the parameter is not for `Adam`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2vNdvtotXck",
        "outputId": "20ce382a-4bb4-499f-f651-23855389faee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024/02/05 12:38:49\tINFO\ttorchdistill.common.main_util\tNot using distributed mode\n",
            "2024/02/05 12:38:49\tINFO\t__main__\tNamespace(config='torchdistill/configs/sample/cifar10/ce/resnet20-hyperparameter_tuning.yaml', device='cuda', run_log='log/cifar10/ce/resnet20-hyperparameter_tuning.log', start_epoch=0, seed=None, disable_cudnn_benchmark=False, test_only=False, student_only=False, log_config=False, world_size=1, dist_url='env://', adjust_lr=False)\n",
            "2024/02/05 12:38:49\tINFO\ttorchdistill.common.main_util\tGetting `CIFAR10` from `torchvision.datasets`\n",
            "2024/02/05 12:38:49\tINFO\ttorchdistill.common.main_util\tCalling `CIFAR10` from `torchvision.datasets` with {'kwargs': {'root': '~/datasets/cifar10', 'train': True, 'download': True}}\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/datasets/cifar10/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:06<00:00, 28276764.88it/s]\n",
            "Extracting /root/datasets/cifar10/cifar-10-python.tar.gz to /root/datasets/cifar10\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `RandomCrop` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `RandomCrop` from `torchvision.transforms` with {'kwargs': {'size': 32, 'padding': 4}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `RandomHorizontalFlip` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `RandomHorizontalFlip` from `torchvision.transforms` with {'kwargs': {'p': 0.5}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `ToTensor` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `ToTensor` from `torchvision.transforms` with {}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `Normalize` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `Normalize` from `torchvision.transforms` with {'kwargs': {'mean': [0.49139968, 0.48215841, 0.44653091], 'std': [0.24703223, 0.24348513, 0.26158784]}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `Compose` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `Compose` from `torchvision.transforms` with {'kwargs': {'transforms': [RandomCrop(size=(32, 32), padding=4), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])]}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `ToTensor` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `ToTensor` from `torchvision.transforms` with {}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `Normalize` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `Normalize` from `torchvision.transforms` with {'kwargs': {'mean': [0.49139968, 0.48215841, 0.44653091], 'std': [0.24703223, 0.24348513, 0.26158784]}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `Compose` from `torchvision.transforms`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `Compose` from `torchvision.transforms` with {'kwargs': {'transforms': [ToTensor(), Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])]}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tGetting `split_dataset` from `torchdistill.datasets.util`\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.common.main_util\tCalling `split_dataset` from `torchdistill.datasets.util` with {'kwargs': {'dataset_id': 'original training dataset', 'dataset': Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: /root/datasets/cifar10\n",
            "    Split: Train, 'lengths': [0.9, 0.1], 'generator_seed': 42, 'sub_splits_configs': [{'transform': Compose(\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])\n",
            ")}, {'transform': Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])\n",
            ")}]}}\n",
            "2024/02/05 12:39:00\tINFO\ttorchdistill.datasets.util\tSplitting `original training dataset` dataset (50000 samples in total)\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tGetting `ToTensor` from `torchvision.transforms`\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tCalling `ToTensor` from `torchvision.transforms` with {}\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tGetting `Normalize` from `torchvision.transforms`\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tCalling `Normalize` from `torchvision.transforms` with {'kwargs': {'mean': [0.49139968, 0.48215841, 0.44653091], 'std': [0.24703223, 0.24348513, 0.26158784]}}\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tGetting `Compose` from `torchvision.transforms`\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tCalling `Compose` from `torchvision.transforms` with {'kwargs': {'transforms': [ToTensor(), Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])]}}\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tGetting `CIFAR10` from `torchvision.datasets`\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tCalling `CIFAR10` from `torchvision.datasets` with {'kwargs': {'root': '~/datasets/cifar10', 'train': False, 'download': True, 'transform': Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])\n",
            ")}}\n",
            "Files already downloaded and verified\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tGetting `RandomSampler` from `torch.utils.data`\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tGetting `SequentialSampler` from `torch.utils.data`\n",
            "2024/02/05 12:39:01\tINFO\ttorchdistill.common.main_util\tckpt file path is None\n",
            "2024/02/05 12:39:02\tINFO\t__main__\tStart training\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "2024/02/05 12:39:02\tINFO\ttorchdistill.core.training\t[student model]\n",
            "2024/02/05 12:39:02\tINFO\ttorchdistill.models.util\tUsing the original model\n",
            "2024/02/05 12:39:02\tINFO\ttorchdistill.core.training\tLoss = 1.0 * CrossEntropyLoss()\n",
            "2024/02/05 12:39:05\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [  0/352]  eta: 0:22:24  lr: 0.1  img/s: 53.06478979709155  loss: 2.6186 (2.6186)  time: 3.8194  data: 1.3634  max mem: 336\n",
            "2024/02/05 12:39:12\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [100/352]  eta: 0:00:26  lr: 0.1  img/s: 1715.65911211668  loss: 1.6998 (1.9237)  time: 0.0656  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:39:18\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [200/352]  eta: 0:00:12  lr: 0.1  img/s: 2502.3813856365127  loss: 1.5523 (1.7928)  time: 0.0566  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:39:26\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [300/352]  eta: 0:00:04  lr: 0.1  img/s: 2370.2386790578576  loss: 1.3689 (1.6804)  time: 0.0579  data: 0.0057  max mem: 336\n",
            "2024/02/05 12:39:28\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:26\n",
            "2024/02/05 12:39:29\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:52  acc1: 46.0938 (46.0938)  acc5: 91.4062 (91.4062)  time: 1.3115  data: 0.7809  max mem: 336\n",
            "2024/02/05 12:39:30\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 12:39:30\tINFO\t__main__\t * Acc@1 52.4200\tAcc@5 94.2400\n",
            "\n",
            "2024/02/05 12:39:30\tINFO\t__main__\tBest top-1 accuracy: 0.0000 -> 52.4200\n",
            "2024/02/05 12:39:30\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:39:31\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [  0/352]  eta: 0:04:50  lr: 0.1  img/s: 1557.7323944070633  loss: 1.2746 (1.2746)  time: 0.8240  data: 0.7415  max mem: 336\n",
            "2024/02/05 12:39:39\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [100/352]  eta: 0:00:22  lr: 0.1  img/s: 2608.119234767739  loss: 1.1353 (1.2218)  time: 0.0575  data: 0.0056  max mem: 336\n",
            "2024/02/05 12:39:45\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [200/352]  eta: 0:00:11  lr: 0.1  img/s: 2534.6935777044414  loss: 1.1057 (1.1895)  time: 0.0585  data: 0.0050  max mem: 336\n",
            "2024/02/05 12:39:52\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 2591.9514894027907  loss: 0.9938 (1.1505)  time: 0.0600  data: 0.0051  max mem: 336\n",
            "2024/02/05 12:39:54\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:23\n",
            "2024/02/05 12:39:54\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:20  acc1: 51.5625 (51.5625)  acc5: 91.4062 (91.4062)  time: 0.5065  data: 0.4281  max mem: 336\n",
            "2024/02/05 12:39:56\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 12:39:56\tINFO\t__main__\t * Acc@1 58.4200\tAcc@5 95.7400\n",
            "\n",
            "2024/02/05 12:39:56\tINFO\t__main__\tBest top-1 accuracy: 52.4200 -> 58.4200\n",
            "2024/02/05 12:39:56\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:39:57\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [  0/352]  eta: 0:06:36  lr: 0.1  img/s: 1043.8237477422126  loss: 0.8021 (0.8021)  time: 1.1277  data: 1.0048  max mem: 336\n",
            "2024/02/05 12:40:04\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [100/352]  eta: 0:00:21  lr: 0.1  img/s: 2007.7220973586684  loss: 0.9816 (0.9808)  time: 0.0564  data: 0.0048  max mem: 336\n",
            "2024/02/05 12:40:10\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 3258.2456591795985  loss: 0.9173 (0.9610)  time: 0.0536  data: 0.0062  max mem: 336\n",
            "2024/02/05 12:40:16\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 1388.6591742087007  loss: 0.8536 (0.9487)  time: 0.0849  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:40:19\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:23\n",
            "2024/02/05 12:40:19\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:27  acc1: 62.5000 (62.5000)  acc5: 96.8750 (96.8750)  time: 0.6780  data: 0.6447  max mem: 336\n",
            "2024/02/05 12:40:21\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 12:40:21\tINFO\t__main__\t * Acc@1 69.4600\tAcc@5 97.6400\n",
            "\n",
            "2024/02/05 12:40:21\tINFO\t__main__\tBest top-1 accuracy: 58.4200 -> 69.4600\n",
            "2024/02/05 12:40:21\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:40:21\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [  0/352]  eta: 0:04:36  lr: 0.1  img/s: 949.5803897222561  loss: 1.0613 (1.0613)  time: 0.7843  data: 0.6492  max mem: 336\n",
            "2024/02/05 12:40:28\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [100/352]  eta: 0:00:18  lr: 0.1  img/s: 2342.338067128266  loss: 0.8342 (0.8321)  time: 0.0658  data: 0.0039  max mem: 336\n",
            "2024/02/05 12:40:34\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 1985.1977059352087  loss: 0.8348 (0.8281)  time: 0.0521  data: 0.0057  max mem: 336\n",
            "2024/02/05 12:40:40\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 3814.9273568347676  loss: 0.7899 (0.8134)  time: 0.0519  data: 0.0050  max mem: 336\n",
            "2024/02/05 12:40:42\tINFO\ttorchdistill.misc.log\tEpoch: [3] Total time: 0:00:21\n",
            "2024/02/05 12:40:43\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:48  acc1: 64.0625 (64.0625)  acc5: 95.3125 (95.3125)  time: 1.2134  data: 1.1542  max mem: 336\n",
            "2024/02/05 12:40:44\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 12:40:44\tINFO\t__main__\t * Acc@1 66.7600\tAcc@5 97.7400\n",
            "\n",
            "2024/02/05 12:40:45\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [  0/352]  eta: 0:06:35  lr: 0.1  img/s: 809.5610159191632  loss: 0.7665 (0.7665)  time: 1.1247  data: 0.9663  max mem: 336\n",
            "2024/02/05 12:40:52\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [100/352]  eta: 0:00:19  lr: 0.1  img/s: 2517.8373845713722  loss: 0.7072 (0.7376)  time: 0.0551  data: 0.0056  max mem: 336\n",
            "2024/02/05 12:40:59\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 2946.860931805208  loss: 0.6529 (0.7192)  time: 0.0530  data: 0.0051  max mem: 336\n",
            "2024/02/05 12:41:04\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 2411.743170699933  loss: 0.7045 (0.7136)  time: 0.0542  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:41:06\tINFO\ttorchdistill.misc.log\tEpoch: [4] Total time: 0:00:21\n",
            "2024/02/05 12:41:07\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:35  acc1: 64.0625 (64.0625)  acc5: 96.8750 (96.8750)  time: 0.8881  data: 0.8332  max mem: 336\n",
            "2024/02/05 12:41:09\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 12:41:09\tINFO\t__main__\t * Acc@1 70.3400\tAcc@5 97.9000\n",
            "\n",
            "2024/02/05 12:41:09\tINFO\t__main__\tBest top-1 accuracy: 69.4600 -> 70.3400\n",
            "2024/02/05 12:41:09\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:41:10\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [  0/352]  eta: 0:07:47  lr: 0.1  img/s: 1106.238691810629  loss: 0.5679 (0.5679)  time: 1.3294  data: 1.2134  max mem: 336\n",
            "2024/02/05 12:41:16\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [100/352]  eta: 0:00:18  lr: 0.1  img/s: 1139.6065225789744  loss: 0.6581 (0.6590)  time: 0.0599  data: 0.0064  max mem: 336\n",
            "2024/02/05 12:41:23\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 2770.2603329239723  loss: 0.6107 (0.6495)  time: 0.0715  data: 0.0055  max mem: 336\n",
            "2024/02/05 12:41:28\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 4328.31261639672  loss: 0.6010 (0.6408)  time: 0.0539  data: 0.0073  max mem: 336\n",
            "2024/02/05 12:41:30\tINFO\ttorchdistill.misc.log\tEpoch: [5] Total time: 0:00:21\n",
            "2024/02/05 12:41:31\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:23  acc1: 62.5000 (62.5000)  acc5: 97.6562 (97.6562)  time: 0.5777  data: 0.5146  max mem: 336\n",
            "2024/02/05 12:41:32\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 12:41:32\tINFO\t__main__\t * Acc@1 73.6000\tAcc@5 98.4400\n",
            "\n",
            "2024/02/05 12:41:32\tINFO\t__main__\tBest top-1 accuracy: 70.3400 -> 73.6000\n",
            "2024/02/05 12:41:32\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:41:33\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [  0/352]  eta: 0:06:07  lr: 0.1  img/s: 821.786893250696  loss: 0.7769 (0.7769)  time: 1.0454  data: 0.8893  max mem: 336\n",
            "2024/02/05 12:41:40\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [100/352]  eta: 0:00:20  lr: 0.1  img/s: 2413.033116392794  loss: 0.5741 (0.6001)  time: 0.0542  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:41:46\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 3025.6987663226946  loss: 0.6015 (0.5965)  time: 0.0539  data: 0.0063  max mem: 336\n",
            "2024/02/05 12:41:52\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 3761.2332525326124  loss: 0.5669 (0.5914)  time: 0.0552  data: 0.0063  max mem: 336\n",
            "2024/02/05 12:41:54\tINFO\ttorchdistill.misc.log\tEpoch: [6] Total time: 0:00:22\n",
            "2024/02/05 12:41:55\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:18  acc1: 64.0625 (64.0625)  acc5: 97.6562 (97.6562)  time: 0.4583  data: 0.4164  max mem: 336\n",
            "2024/02/05 12:41:56\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 12:41:56\tINFO\t__main__\t * Acc@1 72.7800\tAcc@5 97.8600\n",
            "\n",
            "2024/02/05 12:41:57\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [  0/352]  eta: 0:05:46  lr: 0.1  img/s: 1137.6215492325036  loss: 0.5893 (0.5893)  time: 0.9840  data: 0.8541  max mem: 336\n",
            "2024/02/05 12:42:04\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [100/352]  eta: 0:00:20  lr: 0.1  img/s: 3199.7169744974285  loss: 0.5397 (0.5579)  time: 0.0537  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:42:10\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 2452.9886687623366  loss: 0.5272 (0.5621)  time: 0.0546  data: 0.0048  max mem: 336\n",
            "2024/02/05 12:42:16\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 3121.905180584873  loss: 0.5548 (0.5590)  time: 0.0709  data: 0.0065  max mem: 336\n",
            "2024/02/05 12:42:18\tINFO\ttorchdistill.misc.log\tEpoch: [7] Total time: 0:00:21\n",
            "2024/02/05 12:42:19\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:38  acc1: 67.1875 (67.1875)  acc5: 98.4375 (98.4375)  time: 0.9643  data: 0.9364  max mem: 336\n",
            "2024/02/05 12:42:20\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 12:42:20\tINFO\t__main__\t * Acc@1 76.4400\tAcc@5 98.9000\n",
            "\n",
            "2024/02/05 12:42:20\tINFO\t__main__\tBest top-1 accuracy: 73.6000 -> 76.4400\n",
            "2024/02/05 12:42:20\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:42:21\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [  0/352]  eta: 0:07:43  lr: 0.1  img/s: 860.013090720358  loss: 0.5178 (0.5178)  time: 1.3164  data: 1.1673  max mem: 336\n",
            "2024/02/05 12:42:28\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [100/352]  eta: 0:00:20  lr: 0.1  img/s: 1715.65911211668  loss: 0.5386 (0.5251)  time: 0.0815  data: 0.0050  max mem: 336\n",
            "2024/02/05 12:42:34\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 3753.2134532972605  loss: 0.5014 (0.5232)  time: 0.0505  data: 0.0048  max mem: 336\n",
            "2024/02/05 12:42:39\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 1599.2056024378112  loss: 0.4999 (0.5250)  time: 0.0583  data: 0.0053  max mem: 336\n",
            "2024/02/05 12:42:41\tINFO\ttorchdistill.misc.log\tEpoch: [8] Total time: 0:00:21\n",
            "2024/02/05 12:42:42\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:21  acc1: 71.0938 (71.0938)  acc5: 97.6562 (97.6562)  time: 0.5305  data: 0.4981  max mem: 336\n",
            "2024/02/05 12:42:43\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 12:42:43\tINFO\t__main__\t * Acc@1 78.5400\tAcc@5 98.7200\n",
            "\n",
            "2024/02/05 12:42:43\tINFO\t__main__\tBest top-1 accuracy: 76.4400 -> 78.5400\n",
            "2024/02/05 12:42:43\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:42:44\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [  0/352]  eta: 0:04:35  lr: 0.1  img/s: 1074.418707785905  loss: 0.5200 (0.5200)  time: 0.7823  data: 0.6629  max mem: 336\n",
            "2024/02/05 12:42:51\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [100/352]  eta: 0:00:18  lr: 0.1  img/s: 2899.826141439675  loss: 0.5183 (0.4970)  time: 0.0609  data: 0.0046  max mem: 336\n",
            "2024/02/05 12:42:57\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [200/352]  eta: 0:00:10  lr: 0.1  img/s: 3382.6737234739653  loss: 0.4967 (0.4958)  time: 0.0526  data: 0.0055  max mem: 336\n",
            "2024/02/05 12:43:03\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [300/352]  eta: 0:00:03  lr: 0.1  img/s: 2704.4349897992597  loss: 0.4490 (0.4987)  time: 0.0526  data: 0.0049  max mem: 336\n",
            "2024/02/05 12:43:05\tINFO\ttorchdistill.misc.log\tEpoch: [9] Total time: 0:00:21\n",
            "2024/02/05 12:43:06\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:46  acc1: 67.1875 (67.1875)  acc5: 97.6562 (97.6562)  time: 1.1529  data: 1.0704  max mem: 336\n",
            "2024/02/05 12:43:07\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 12:43:07\tINFO\t__main__\t * Acc@1 78.7000\tAcc@5 99.1000\n",
            "\n",
            "2024/02/05 12:43:07\tINFO\t__main__\tBest top-1 accuracy: 78.5400 -> 78.7000\n",
            "2024/02/05 12:43:07\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/ce/cifar10-resnet20-hyperparameter_tuning.pt\n",
            "2024/02/05 12:43:07\tINFO\t__main__\tTraining time 0:04:05\n",
            "2024/02/05 12:43:07\tINFO\ttorchdistill.common.main_util\tLoading model parameters\n",
            "2024/02/05 12:43:07\tINFO\t__main__\t[Student: resnet20]\n",
            "2024/02/05 12:43:08\tINFO\ttorchdistill.misc.log\tTest:  [    0/10000]  eta: 0:58:01  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.3481  data: 0.2726  max mem: 336\n",
            "2024/02/05 12:43:13\tINFO\ttorchdistill.misc.log\tTest:  [ 1000/10000]  eta: 0:00:51  acc1: 100.0000 (80.5195)  acc5: 100.0000 (99.2008)  time: 0.0054  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:43:19\tINFO\ttorchdistill.misc.log\tTest:  [ 2000/10000]  eta: 0:00:46  acc1: 100.0000 (78.9605)  acc5: 100.0000 (98.9505)  time: 0.0054  data: 0.0003  max mem: 336\n",
            "2024/02/05 12:43:25\tINFO\ttorchdistill.misc.log\tTest:  [ 3000/10000]  eta: 0:00:40  acc1: 100.0000 (78.2073)  acc5: 100.0000 (98.8670)  time: 0.0053  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:43:30\tINFO\ttorchdistill.misc.log\tTest:  [ 4000/10000]  eta: 0:00:34  acc1: 100.0000 (78.1305)  acc5: 100.0000 (98.8253)  time: 0.0069  data: 0.0017  max mem: 336\n",
            "2024/02/05 12:43:36\tINFO\ttorchdistill.misc.log\tTest:  [ 5000/10000]  eta: 0:00:28  acc1: 100.0000 (78.1044)  acc5: 100.0000 (98.8802)  time: 0.0056  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:43:41\tINFO\ttorchdistill.misc.log\tTest:  [ 6000/10000]  eta: 0:00:22  acc1: 100.0000 (78.2536)  acc5: 100.0000 (98.9335)  time: 0.0052  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:43:47\tINFO\ttorchdistill.misc.log\tTest:  [ 7000/10000]  eta: 0:00:17  acc1: 100.0000 (78.2888)  acc5: 100.0000 (98.9287)  time: 0.0055  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:43:53\tINFO\ttorchdistill.misc.log\tTest:  [ 8000/10000]  eta: 0:00:11  acc1: 100.0000 (78.1902)  acc5: 100.0000 (98.9376)  time: 0.0050  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:43:59\tINFO\ttorchdistill.misc.log\tTest:  [ 9000/10000]  eta: 0:00:05  acc1: 100.0000 (78.2358)  acc5: 100.0000 (98.8890)  time: 0.0052  data: 0.0002  max mem: 336\n",
            "2024/02/05 12:44:04\tINFO\ttorchdistill.misc.log\tTest: Total time: 0:00:56\n",
            "2024/02/05 12:44:04\tINFO\t__main__\t * Acc@1 78.3400\tAcc@5 98.9400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python torchdistill/examples/torchvision/image_classification.py --config torchdistill/configs/sample/cifar10/ce/resnet20-hyperparameter_tuning.yaml --run_log log/cifar10/ce/resnet20-hyperparameter_tuning.log"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
