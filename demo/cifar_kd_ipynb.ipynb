{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piAVeOax6ph6"
      },
      "source": [
        "# Distilling knowledge in models pretrained on CIFAR-10/100 datasets, using ***torchdistill***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZF7kY-6zDh"
      },
      "source": [
        "## 1. Make sure you have access to GPU/TPU\n",
        "Google Colab: *Runtime* -> *Change runtime type* -> *Hardware accelarator*: \"GPU\" or \"TPU\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy7V9yQH6o3J",
        "outputId": "c5e5958e-2b84-465d-a4a2-cf030259fec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb  5 12:57:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur2bTpuW64r5"
      },
      "source": [
        "## 2. Install ***torchdistill***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJSCmbJH_yuC",
        "outputId": "a911c597-ea4a-4e42-cf05-a1730c2b4b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdistill\n",
            "  Downloading torchdistill-1.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchdistill) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision<=0.16.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from torchdistill) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchdistill) (1.23.5)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from torchdistill) (6.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchdistill) (1.11.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torchdistill) (3.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<=2.1.0,>=2.0.0->torchdistill) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision<=0.16.0,>=0.15.1->torchdistill) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<=0.16.0,>=0.15.1->torchdistill) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.1.0,>=2.0.0->torchdistill) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision<=0.16.0,>=0.15.1->torchdistill) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.1.0,>=2.0.0->torchdistill) (1.3.0)\n",
            "Installing collected packages: torchdistill\n",
            "Successfully installed torchdistill-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdistill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOKnpfrD7DFC"
      },
      "source": [
        "## 3. Clone ***torchdistill*** repository to use its example code and configuration files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa4hqLn57Hkz",
        "outputId": "bc74eeab-6703-4eb0-ea9a-97526c4d6d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdistill'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 151 (delta 14), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (151/151), 319.69 KiB | 15.22 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Moon-kimchi/torchdistill.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGrp0OmZ7a_u"
      },
      "source": [
        "## 4. Distill knowledge in models pretrained on CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VETfswVJ7Xj0"
      },
      "source": [
        "Note that the hyperparameters of ResNet were chosen based on either train/val (splitting 50k samples into train:val = 45k:5k) or cross-validation, according to the original papers.  \n",
        "For the final run (once the hyperparameters are finalized), the authors used all the training images (50k samples).  \n",
        "- ResNet: https://github.com/facebookarchive/fb.resnet.torch\n",
        "\n",
        "The following examples demonstrate how to 1) tune hyperparameter and 2) do final-run with ResNet-20 on CIFAR-10 dataset, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TpYnaQs781n"
      },
      "source": [
        "### 4.1 Hyperparameter tuning based on train:val = 45k:5k\n",
        "Let's start with a small **student model**, ResNet-20, with a pretrained DenseNet-BC (k=12, depth=100) as a **teacher model** for tutorial.  \n",
        "\n",
        "Open `torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.yaml` and update hyperparameters as you wish e.g., number of epochs (*num_epochs*), batch size (*batch_size* in *train_data_loader* entry), learning rate (*lr* within *optimizer* entry), and so on.\n",
        "By default, the hyperparameters in the example config are identical to those in the final run config.\n",
        "  \n",
        "You will find a lot of module names from [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [torchvision](https://pytorch.org/docs/stable/torchvision/) such as [`SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [`MultiStepLR`](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiStepLR), [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), [`CIFAR10`](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.CIFAR10), [`RandomCrop`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomCrop) (, and more). You can update their parameters or replace such modules with other modules in the packages. For instance, `SGD` could be replaced with [`Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), and then you will change the parameters under `params` (at least delete `momentum` entry as the parameter is not for `Adam`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy9Yr2tB8avA",
        "outputId": "81442d62-2aff-4191-88ac-017c2d0cd83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024/02/05 12:57:56\tINFO\ttorchdistill.common.main_util\tNot using distributed mode\n",
            "2024/02/05 12:57:56\tINFO\t__main__\tNamespace(config='torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.yaml', device='cuda', run_log='log/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.log', start_epoch=0, seed=None, disable_cudnn_benchmark=False, test_only=False, student_only=False, log_config=False, world_size=1, dist_url='env://', adjust_lr=False)\n",
            "2024/02/05 12:57:56\tINFO\ttorchdistill.common.main_util\tGetting `CIFAR10` from `torchvision.datasets`\n",
            "2024/02/05 12:57:56\tINFO\ttorchdistill.common.main_util\tCalling `CIFAR10` from `torchvision.datasets` with {'kwargs': {'root': '~/datasets/cifar10', 'train': True, 'download': True}}\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /root/datasets/cifar10/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:13<00:00, 13027330.94it/s]\n",
            "Extracting /root/datasets/cifar10/cifar-10-python.tar.gz to /root/datasets/cifar10\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `RandomCrop` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `RandomCrop` from `torchvision.transforms` with {'kwargs': {'size': 32, 'padding': 4}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `RandomHorizontalFlip` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `RandomHorizontalFlip` from `torchvision.transforms` with {'kwargs': {'p': 0.5}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `ToTensor` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `ToTensor` from `torchvision.transforms` with {}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `Normalize` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `Normalize` from `torchvision.transforms` with {'kwargs': {'mean': [0.49139968, 0.48215841, 0.44653091], 'std': [0.24703223, 0.24348513, 0.26158784]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `Compose` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `Compose` from `torchvision.transforms` with {'kwargs': {'transforms': [RandomCrop(size=(32, 32), padding=4), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `ToTensor` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `ToTensor` from `torchvision.transforms` with {}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `Normalize` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `Normalize` from `torchvision.transforms` with {'kwargs': {'mean': [0.49139968, 0.48215841, 0.44653091], 'std': [0.24703223, 0.24348513, 0.26158784]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `Compose` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `Compose` from `torchvision.transforms` with {'kwargs': {'transforms': [ToTensor(), Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `split_dataset` from `torchdistill.datasets.util`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `split_dataset` from `torchdistill.datasets.util` with {'kwargs': {'dataset_id': 'original training dataset', 'dataset': Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: /root/datasets/cifar10\n",
            "    Split: Train, 'lengths': [0.9, 0.1], 'generator_seed': 42, 'sub_splits_configs': [{'transform': Compose(\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])\n",
            ")}, {'transform': Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])\n",
            ")}]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.datasets.util\tSplitting `original training dataset` dataset (50000 samples in total)\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `ToTensor` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `ToTensor` from `torchvision.transforms` with {}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `Normalize` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `Normalize` from `torchvision.transforms` with {'kwargs': {'mean': [0.49139968, 0.48215841, 0.44653091], 'std': [0.24703223, 0.24348513, 0.26158784]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `Compose` from `torchvision.transforms`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `Compose` from `torchvision.transforms` with {'kwargs': {'transforms': [ToTensor(), Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])]}}\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tGetting `CIFAR10` from `torchvision.datasets`\n",
            "2024/02/05 12:58:13\tINFO\ttorchdistill.common.main_util\tCalling `CIFAR10` from `torchvision.datasets` with {'kwargs': {'root': '~/datasets/cifar10', 'train': False, 'download': True, 'transform': Compose(\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.49139968, 0.48215841, 0.44653091], std=[0.24703223, 0.24348513, 0.26158784])\n",
            ")}}\n",
            "Files already downloaded and verified\n",
            "2024/02/05 12:58:14\tINFO\ttorchdistill.common.main_util\tGetting `RandomSampler` from `torch.utils.data`\n",
            "2024/02/05 12:58:14\tINFO\ttorchdistill.common.main_util\tGetting `SequentialSampler` from `torch.utils.data`\n",
            "Downloading: \"https://github.com/yoshitomo-matsubara/torchdistill/releases/download/v0.1.1/cifar10-densenet_bc_k12_depth100.pt\" to /root/.cache/torch/hub/checkpoints/cifar10-densenet_bc_k12_depth100.pt\n",
            "100% 3.23M/3.23M [00:00<00:00, 9.81MB/s]\n",
            "2024/02/05 12:58:16\tINFO\ttorchdistill.common.main_util\tckpt file path is None\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.common.main_util\tckpt file path is None\n",
            "2024/02/05 12:58:17\tINFO\t__main__\tStart training\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.core.distillation\t[teacher model]\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.models.util\tUsing the original model\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.core.distillation\t[student model]\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.models.util\tUsing the original model\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.core.distillation\tLoss = 1.0 * KDLoss(\n",
            "  (cross_entropy_loss): CrossEntropyLoss()\n",
            ")\n",
            "2024/02/05 12:58:17\tINFO\ttorchdistill.core.distillation\tFreezing the whole teacher model\n",
            "2024/02/05 12:58:21\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [  0/352]  eta: 0:26:56  lr: 0.1  img/s: 38.837111771508795  loss: 4.8414 (4.8414)  time: 4.5918  data: 1.2531  max mem: 700\n",
            "2024/02/05 12:58:33\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [100/352]  eta: 0:00:41  lr: 0.1  img/s: 1074.683246523955  loss: 3.6052 (3.9285)  time: 0.1191  data: 0.0002  max mem: 700\n",
            "2024/02/05 12:58:46\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [200/352]  eta: 0:00:21  lr: 0.1  img/s: 1075.1222810088152  loss: 3.2556 (3.6550)  time: 0.1200  data: 0.0002  max mem: 700\n",
            "2024/02/05 12:58:58\tINFO\ttorchdistill.misc.log\tEpoch: [0]  [300/352]  eta: 0:00:07  lr: 0.1  img/s: 1066.7482226522916  loss: 2.8886 (3.4559)  time: 0.1207  data: 0.0002  max mem: 700\n",
            "2024/02/05 12:59:05\tINFO\ttorchdistill.misc.log\tEpoch: [0] Total time: 0:00:48\n",
            "2024/02/05 12:59:06\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:58  acc1: 42.1875 (42.1875)  acc5: 86.7188 (86.7188)  time: 1.4581  data: 0.7380  max mem: 700\n",
            "2024/02/05 12:59:07\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 12:59:07\tINFO\t__main__\t * Acc@1 50.0200\tAcc@5 92.1000\n",
            "\n",
            "2024/02/05 12:59:07\tINFO\t__main__\tBest top-1 accuracy: 0.0000 -> 50.0200\n",
            "2024/02/05 12:59:07\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 12:59:08\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [  0/352]  eta: 0:08:16  lr: 0.1  img/s: 274.1424157396604  loss: 3.0376 (3.0376)  time: 1.4097  data: 0.9425  max mem: 700\n",
            "2024/02/05 12:59:21\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1054.8992142353286  loss: 2.4940 (2.6030)  time: 0.1223  data: 0.0002  max mem: 700\n",
            "2024/02/05 12:59:33\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1054.232202853984  loss: 2.2715 (2.5009)  time: 0.1248  data: 0.0010  max mem: 700\n",
            "2024/02/05 12:59:46\tINFO\ttorchdistill.misc.log\tEpoch: [1]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1027.561169902253  loss: 2.2356 (2.4147)  time: 0.1269  data: 0.0021  max mem: 700\n",
            "2024/02/05 12:59:52\tINFO\ttorchdistill.misc.log\tEpoch: [1] Total time: 0:00:45\n",
            "2024/02/05 12:59:53\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:18  acc1: 52.3438 (52.3438)  acc5: 98.4375 (98.4375)  time: 0.4606  data: 0.4164  max mem: 700\n",
            "2024/02/05 12:59:55\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 12:59:55\tINFO\t__main__\t * Acc@1 60.2200\tAcc@5 96.3800\n",
            "\n",
            "2024/02/05 12:59:55\tINFO\t__main__\tBest top-1 accuracy: 50.0200 -> 60.2200\n",
            "2024/02/05 12:59:55\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 12:59:57\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [  0/352]  eta: 0:12:32  lr: 0.1  img/s: 198.51147594567982  loss: 2.1897 (2.1897)  time: 2.1367  data: 1.4916  max mem: 700\n",
            "2024/02/05 13:00:10\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [100/352]  eta: 0:00:36  lr: 0.1  img/s: 1043.0814867776576  loss: 2.0181 (2.0580)  time: 0.1297  data: 0.0039  max mem: 700\n",
            "2024/02/05 13:00:22\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1031.8210003959148  loss: 1.9854 (2.0064)  time: 0.1331  data: 0.0051  max mem: 700\n",
            "2024/02/05 13:00:35\tINFO\ttorchdistill.misc.log\tEpoch: [2]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1038.700232941486  loss: 1.9262 (1.9635)  time: 0.1307  data: 0.0041  max mem: 700\n",
            "2024/02/05 13:00:41\tINFO\ttorchdistill.misc.log\tEpoch: [2] Total time: 0:00:46\n",
            "2024/02/05 13:00:42\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:19  acc1: 54.6875 (54.6875)  acc5: 96.0938 (96.0938)  time: 0.4962  data: 0.4722  max mem: 700\n",
            "2024/02/05 13:00:43\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:00:43\tINFO\t__main__\t * Acc@1 65.2000\tAcc@5 97.6000\n",
            "\n",
            "2024/02/05 13:00:43\tINFO\t__main__\tBest top-1 accuracy: 60.2200 -> 65.2000\n",
            "2024/02/05 13:00:43\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:00:44\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [  0/352]  eta: 0:08:11  lr: 0.1  img/s: 219.01432583868592  loss: 1.7159 (1.7159)  time: 1.3974  data: 0.8127  max mem: 700\n",
            "2024/02/05 13:00:57\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1045.179167704959  loss: 1.6257 (1.7022)  time: 0.1259  data: 0.0015  max mem: 700\n",
            "2024/02/05 13:01:10\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1036.9928706499086  loss: 1.7233 (1.7056)  time: 0.1234  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:01:22\tINFO\ttorchdistill.misc.log\tEpoch: [3]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1035.1513805337036  loss: 1.5816 (1.6650)  time: 0.1236  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:01:29\tINFO\ttorchdistill.misc.log\tEpoch: [3] Total time: 0:00:45\n",
            "2024/02/05 13:01:30\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:31  acc1: 60.9375 (60.9375)  acc5: 96.0938 (96.0938)  time: 0.7895  data: 0.7193  max mem: 700\n",
            "2024/02/05 13:01:30\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:01:30\tINFO\t__main__\t * Acc@1 72.0800\tAcc@5 98.2000\n",
            "\n",
            "2024/02/05 13:01:30\tINFO\t__main__\tBest top-1 accuracy: 65.2000 -> 72.0800\n",
            "2024/02/05 13:01:30\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:01:32\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [  0/352]  eta: 0:09:34  lr: 0.1  img/s: 265.0058823732664  loss: 1.6420 (1.6420)  time: 1.6331  data: 1.1499  max mem: 700\n",
            "2024/02/05 13:01:45\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1037.8428401589035  loss: 1.4082 (1.5056)  time: 0.1233  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:01:57\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1005.6832179110852  loss: 1.5396 (1.4936)  time: 0.1237  data: 0.0005  max mem: 700\n",
            "2024/02/05 13:02:10\tINFO\ttorchdistill.misc.log\tEpoch: [4]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1047.121891279883  loss: 1.4210 (1.4810)  time: 0.1235  data: 0.0005  max mem: 700\n",
            "2024/02/05 13:02:16\tINFO\ttorchdistill.misc.log\tEpoch: [4] Total time: 0:00:45\n",
            "2024/02/05 13:02:17\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:18  acc1: 61.7188 (61.7188)  acc5: 97.6562 (97.6562)  time: 0.4664  data: 0.3891  max mem: 700\n",
            "2024/02/05 13:02:18\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:02:18\tINFO\t__main__\t * Acc@1 71.8600\tAcc@5 98.7200\n",
            "\n",
            "2024/02/05 13:02:19\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [  0/352]  eta: 0:07:37  lr: 0.1  img/s: 217.02039471636456  loss: 1.5996 (1.5996)  time: 1.2993  data: 0.6972  max mem: 700\n",
            "2024/02/05 13:02:32\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1037.5901094080064  loss: 1.3426 (1.3693)  time: 0.1238  data: 0.0005  max mem: 700\n",
            "2024/02/05 13:02:45\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1046.4524373390716  loss: 1.2822 (1.3479)  time: 0.1235  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:02:57\tINFO\ttorchdistill.misc.log\tEpoch: [5]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1033.1275151253906  loss: 1.2375 (1.3346)  time: 0.1238  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:03:04\tINFO\ttorchdistill.misc.log\tEpoch: [5] Total time: 0:00:45\n",
            "2024/02/05 13:03:04\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:23  acc1: 64.8438 (64.8438)  acc5: 96.8750 (96.8750)  time: 0.5940  data: 0.5319  max mem: 700\n",
            "2024/02/05 13:03:06\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:02\n",
            "2024/02/05 13:03:06\tINFO\t__main__\t * Acc@1 74.4800\tAcc@5 98.5200\n",
            "\n",
            "2024/02/05 13:03:06\tINFO\t__main__\tBest top-1 accuracy: 72.0800 -> 74.4800\n",
            "2024/02/05 13:03:06\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:03:08\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [  0/352]  eta: 0:10:52  lr: 0.1  img/s: 209.77747768868883  loss: 1.2457 (1.2457)  time: 1.8541  data: 1.2436  max mem: 700\n",
            "2024/02/05 13:03:21\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [100/352]  eta: 0:00:36  lr: 0.1  img/s: 1031.5949795168592  loss: 1.1986 (1.2066)  time: 0.1275  data: 0.0030  max mem: 700\n",
            "2024/02/05 13:03:33\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1035.436872344488  loss: 1.2128 (1.2300)  time: 0.1283  data: 0.0019  max mem: 700\n",
            "2024/02/05 13:03:46\tINFO\ttorchdistill.misc.log\tEpoch: [6]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1023.9589860177872  loss: 1.2122 (1.2324)  time: 0.1320  data: 0.0058  max mem: 700\n",
            "2024/02/05 13:03:52\tINFO\ttorchdistill.misc.log\tEpoch: [6] Total time: 0:00:46\n",
            "2024/02/05 13:03:52\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:15  acc1: 66.4062 (66.4062)  acc5: 96.8750 (96.8750)  time: 0.3900  data: 0.3514  max mem: 700\n",
            "2024/02/05 13:03:54\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:03:54\tINFO\t__main__\t * Acc@1 77.6200\tAcc@5 98.3400\n",
            "\n",
            "2024/02/05 13:03:54\tINFO\t__main__\tBest top-1 accuracy: 74.4800 -> 77.6200\n",
            "2024/02/05 13:03:54\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:03:55\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [  0/352]  eta: 0:08:57  lr: 0.1  img/s: 224.05189571778533  loss: 0.9634 (0.9634)  time: 1.5264  data: 0.9549  max mem: 700\n",
            "2024/02/05 13:04:08\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1043.8440429226107  loss: 1.1344 (1.1796)  time: 0.1236  data: 0.0005  max mem: 700\n",
            "2024/02/05 13:04:21\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1036.8827171700395  loss: 1.0826 (1.1700)  time: 0.1235  data: 0.0004  max mem: 700\n",
            "2024/02/05 13:04:33\tINFO\ttorchdistill.misc.log\tEpoch: [7]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1036.2642896987547  loss: 1.0733 (1.1496)  time: 0.1237  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:04:40\tINFO\ttorchdistill.misc.log\tEpoch: [7] Total time: 0:00:45\n",
            "2024/02/05 13:04:40\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:27  acc1: 68.7500 (68.7500)  acc5: 97.6562 (97.6562)  time: 0.6975  data: 0.6264  max mem: 700\n",
            "2024/02/05 13:04:41\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:04:41\tINFO\t__main__\t * Acc@1 79.3000\tAcc@5 98.5600\n",
            "\n",
            "2024/02/05 13:04:41\tINFO\t__main__\tBest top-1 accuracy: 77.6200 -> 79.3000\n",
            "2024/02/05 13:04:41\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:04:43\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [  0/352]  eta: 0:08:30  lr: 0.1  img/s: 238.72388168543122  loss: 0.9791 (0.9791)  time: 1.4493  data: 0.9128  max mem: 700\n",
            "2024/02/05 13:04:56\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1042.0307443868203  loss: 1.1284 (1.0986)  time: 0.1234  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:05:08\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1034.744377886224  loss: 1.0619 (1.1056)  time: 0.1235  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:05:21\tINFO\ttorchdistill.misc.log\tEpoch: [8]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1042.6236777149204  loss: 1.1501 (1.1160)  time: 0.1236  data: 0.0003  max mem: 700\n",
            "2024/02/05 13:05:27\tINFO\ttorchdistill.misc.log\tEpoch: [8] Total time: 0:00:45\n",
            "2024/02/05 13:05:28\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:27  acc1: 69.5312 (69.5312)  acc5: 98.4375 (98.4375)  time: 0.6772  data: 0.6407  max mem: 700\n",
            "2024/02/05 13:05:29\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:05:29\tINFO\t__main__\t * Acc@1 80.0600\tAcc@5 99.1400\n",
            "\n",
            "2024/02/05 13:05:29\tINFO\t__main__\tBest top-1 accuracy: 79.3000 -> 80.0600\n",
            "2024/02/05 13:05:29\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:05:31\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [  0/352]  eta: 0:07:52  lr: 0.1  img/s: 240.20607746696595  loss: 1.1428 (1.1428)  time: 1.3434  data: 0.8103  max mem: 700\n",
            "2024/02/05 13:05:43\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [100/352]  eta: 0:00:35  lr: 0.1  img/s: 1033.989079836602  loss: 1.0479 (1.0529)  time: 0.1232  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:05:56\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [200/352]  eta: 0:00:20  lr: 0.1  img/s: 1043.3389276483276  loss: 1.0848 (1.0462)  time: 0.1245  data: 0.0012  max mem: 700\n",
            "2024/02/05 13:06:08\tINFO\ttorchdistill.misc.log\tEpoch: [9]  [300/352]  eta: 0:00:06  lr: 0.1  img/s: 1036.734547589254  loss: 0.9579 (1.0433)  time: 0.1245  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:06:15\tINFO\ttorchdistill.misc.log\tEpoch: [9] Total time: 0:00:45\n",
            "2024/02/05 13:06:15\tINFO\ttorchdistill.misc.log\tValidation:  [ 0/40]  eta: 0:00:30  acc1: 74.2188 (74.2188)  acc5: 98.4375 (98.4375)  time: 0.7704  data: 0.7222  max mem: 700\n",
            "2024/02/05 13:06:16\tINFO\ttorchdistill.misc.log\tValidation: Total time: 0:00:01\n",
            "2024/02/05 13:06:16\tINFO\t__main__\t * Acc@1 81.3600\tAcc@5 98.8600\n",
            "\n",
            "2024/02/05 13:06:16\tINFO\t__main__\tBest top-1 accuracy: 80.0600 -> 81.3600\n",
            "2024/02/05 13:06:16\tINFO\t__main__\tUpdating ckpt at ./resource/ckpt/cifar10/kd/cifar10-resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.pt\n",
            "2024/02/05 13:06:17\tINFO\t__main__\tTraining time 0:07:59\n",
            "2024/02/05 13:06:17\tINFO\ttorchdistill.common.main_util\tLoading model parameters\n",
            "2024/02/05 13:06:17\tINFO\t__main__\t[Teacher: densenet_bc_k12_depth100]\n",
            "2024/02/05 13:06:17\tINFO\ttorchdistill.misc.log\tTest:  [    0/10000]  eta: 1:52:09  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.6729  data: 0.4145  max mem: 700\n",
            "2024/02/05 13:06:35\tINFO\ttorchdistill.misc.log\tTest:  [ 1000/10000]  eta: 0:02:50  acc1: 100.0000 (95.1049)  acc5: 100.0000 (99.9001)  time: 0.0167  data: 0.0001  max mem: 700\n",
            "2024/02/05 13:06:54\tINFO\ttorchdistill.misc.log\tTest:  [ 2000/10000]  eta: 0:02:27  acc1: 100.0000 (95.2024)  acc5: 100.0000 (99.8501)  time: 0.0242  data: 0.0015  max mem: 700\n",
            "2024/02/05 13:07:12\tINFO\ttorchdistill.misc.log\tTest:  [ 3000/10000]  eta: 0:02:08  acc1: 100.0000 (95.3016)  acc5: 100.0000 (99.8001)  time: 0.0172  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:07:30\tINFO\ttorchdistill.misc.log\tTest:  [ 4000/10000]  eta: 0:01:49  acc1: 100.0000 (95.3762)  acc5: 100.0000 (99.8250)  time: 0.0234  data: 0.0018  max mem: 700\n",
            "2024/02/05 13:07:48\tINFO\ttorchdistill.misc.log\tTest:  [ 5000/10000]  eta: 0:01:31  acc1: 100.0000 (95.3009)  acc5: 100.0000 (99.8200)  time: 0.0169  data: 0.0001  max mem: 700\n",
            "2024/02/05 13:08:06\tINFO\ttorchdistill.misc.log\tTest:  [ 6000/10000]  eta: 0:01:12  acc1: 100.0000 (95.3841)  acc5: 100.0000 (99.8500)  time: 0.0245  data: 0.0012  max mem: 700\n",
            "2024/02/05 13:08:25\tINFO\ttorchdistill.misc.log\tTest:  [ 7000/10000]  eta: 0:00:54  acc1: 100.0000 (95.4149)  acc5: 100.0000 (99.8714)  time: 0.0240  data: 0.0016  max mem: 700\n",
            "2024/02/05 13:08:43\tINFO\ttorchdistill.misc.log\tTest:  [ 8000/10000]  eta: 0:00:36  acc1: 100.0000 (95.3256)  acc5: 100.0000 (99.8625)  time: 0.0229  data: 0.0018  max mem: 700\n",
            "2024/02/05 13:09:01\tINFO\ttorchdistill.misc.log\tTest:  [ 9000/10000]  eta: 0:00:18  acc1: 100.0000 (95.3450)  acc5: 100.0000 (99.8556)  time: 0.0178  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:09:20\tINFO\ttorchdistill.misc.log\tTest: Total time: 0:03:03\n",
            "2024/02/05 13:09:20\tINFO\t__main__\t * Acc@1 95.3600\tAcc@5 99.8700\n",
            "\n",
            "2024/02/05 13:09:20\tINFO\t__main__\t[Student: resnet20]\n",
            "2024/02/05 13:09:21\tINFO\ttorchdistill.misc.log\tTest:  [    0/10000]  eta: 1:36:11  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 0.5771  data: 0.4525  max mem: 700\n",
            "2024/02/05 13:09:26\tINFO\ttorchdistill.misc.log\tTest:  [ 1000/10000]  eta: 0:00:54  acc1: 100.0000 (81.9181)  acc5: 100.0000 (98.9011)  time: 0.0053  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:09:32\tINFO\ttorchdistill.misc.log\tTest:  [ 2000/10000]  eta: 0:00:46  acc1: 100.0000 (81.1094)  acc5: 100.0000 (98.5507)  time: 0.0074  data: 0.0017  max mem: 700\n",
            "2024/02/05 13:09:37\tINFO\ttorchdistill.misc.log\tTest:  [ 3000/10000]  eta: 0:00:40  acc1: 100.0000 (80.8730)  acc5: 100.0000 (98.7338)  time: 0.0053  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:09:43\tINFO\ttorchdistill.misc.log\tTest:  [ 4000/10000]  eta: 0:00:34  acc1: 100.0000 (80.9048)  acc5: 100.0000 (98.7003)  time: 0.0051  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:09:49\tINFO\ttorchdistill.misc.log\tTest:  [ 5000/10000]  eta: 0:00:28  acc1: 100.0000 (81.1238)  acc5: 100.0000 (98.7802)  time: 0.0053  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:09:54\tINFO\ttorchdistill.misc.log\tTest:  [ 6000/10000]  eta: 0:00:22  acc1: 100.0000 (81.2698)  acc5: 100.0000 (98.7669)  time: 0.0052  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:10:00\tINFO\ttorchdistill.misc.log\tTest:  [ 7000/10000]  eta: 0:00:17  acc1: 100.0000 (81.2170)  acc5: 100.0000 (98.8287)  time: 0.0059  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:10:06\tINFO\ttorchdistill.misc.log\tTest:  [ 8000/10000]  eta: 0:00:11  acc1: 100.0000 (81.3773)  acc5: 100.0000 (98.8876)  time: 0.0056  data: 0.0004  max mem: 700\n",
            "2024/02/05 13:10:12\tINFO\ttorchdistill.misc.log\tTest:  [ 9000/10000]  eta: 0:00:05  acc1: 100.0000 (81.2910)  acc5: 100.0000 (98.8224)  time: 0.0060  data: 0.0002  max mem: 700\n",
            "2024/02/05 13:10:18\tINFO\ttorchdistill.misc.log\tTest: Total time: 0:00:57\n",
            "2024/02/05 13:10:18\tINFO\t__main__\t * Acc@1 81.2900\tAcc@5 98.8700\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python torchdistill/examples/torchvision/image_classification.py --config torchdistill/configs/sample/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.yaml --run_log log/cifar10/kd/resnet20_from_densenet_bc_k12_depth100-hyperparameter_tuning.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUTCWU7PGB31"
      },
      "source": [
        "At the end of the training process, you will see improved accuracy of the student model (ResNet-20) compared to that trained without teacher in another example notebook and/or the accuracy reported in [the ResNet paper (Table 6)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}